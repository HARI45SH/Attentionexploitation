import torch
import torch.nn as nn
from einops import rearrange
from transformers import ViTConfig,ViTModel
from torchvision.models import resnet18,ResNet18_Weights


class Attention_exploitation(nn.Module):
    def __init__(self,type="base",num_classes=2):
        super().__init__()
        self.num_classes=num_classes
        config=ViTConfig.from_pretrained(f"google/vit-{type}-patch16-224")
        self.model=ViTModel(config)
        self.classifier=resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)
        self.dim=1024 if type=="large" else 768
        self.classifier.fc=nn.Linear(self.classifier.fc.in_features,self.dim)
        self.stemming_block=nn.Sequential(
            nn.ConvTranspose2d(self.dim,256,16,3),
            nn.BatchNorm2d(256),
            nn.GELU(),
            nn.ConvTranspose2d(256,128,kernel_size=7,stride=2,padding=1),
            nn.BatchNorm2d(128),
            nn.GELU(),
            nn.ConvTranspose2d(128,3,3,2),
            nn.BatchNorm2d(3),
            nn.GELU(),
            nn.Conv2d(3,3,4),
            nn.BatchNorm2d(3),
            nn.GELU()
        )
        self.Linearlayer=nn.Linear(self.dim,self.dim)
        self.norm1=nn.InstanceNorm2d(3)
        self.norm2=nn.LayerNorm(self.dim)
        self.norm3=nn.LayerNorm(self.dim)
        self.dropout=nn.Dropout(p=0.4)
        self.gelu=nn.GELU()
        self.mlp=nn.Sequential(
            nn.SiLU(),
            nn.Linear(self.dim,self.num_classes)
        )

        self.freeze_gradients(self.model,self.classifier)
    def freeze_gradients(self,vit_model,resnet_model):
        for i in vit_model.parameters():
            i.requires_grad=False

        for i in resnet_model.parameters():
            i.requires_grad=False
        
        for i in resnet_model.fc.parameters():
            i.requires_grad=True
        
        for i in self.stemming_block.parameters():
            i.requires_grad=False
        print("parameters are frozen!!")
    
    def forward(self,x):
        x1=self.model(x).last_hidden_state
        x1_class,x1_representation=x1[:,0,:],x1[:,1:,:]
        x1_representation=rearrange(x1_representation,'b (h w) d-> b d h w ',h=14)
        x1_rep_image=self.stemming_block(x1_representation)


        x=x+x1_rep_image

        x=self.classifier(x)

        x+=x1_class
        x=self.gelu(x)

        x=self.Linearlayer(x)
        x=self.gelu(x)
        x=self.norm3(x)

        x=self.dropout(x)

        x=self.mlp(x)

        return x