{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from attention_tuning import Attention_exploitation\n",
    "from PIL import Image\n",
    "from torchmetrics.classification import BinaryAccuracy,BinaryF1Score\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreakHis(Dataset):\n",
    "    def __init__(self,X,y,transforms=None):\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.transforms=transforms\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        path=self.X[idx]\n",
    "        image=Image.open(path)\n",
    "        image=self.transforms(image)\n",
    "        \n",
    "        return{\n",
    "           'Image' :image,\n",
    "           'class':self.y[idx],\n",
    "           'magnification':self.X[idx].split('\\\\')[-2]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_names(directory):\n",
    "    file_paths=glob.glob(directory+\"/**/*.png\",recursive=True)\n",
    "    classes=[get_class_properties(i) for i in file_paths]\n",
    "    return file_paths,classes\n",
    "def get_class_properties(directory):\n",
    "    directory=directory.replace('\\\\','/')\n",
    "    d=directory.split('/')[-2]\n",
    "    return 0 if d=='benign' else 1\n",
    "def get_count(directory):\n",
    "    file_paths=[x.replace('\\\\','/') for x in glob.glob(directory+\"/**/*.png\",recursive=True)]\n",
    "    total=len(file_paths)\n",
    "    benin_count=len([i for i in file_paths if i.split('/')[-2]==\"benign\"])\n",
    "    print(total)\n",
    "    malignant_count=total-benin_count\n",
    "    return benin_count/total,malignant_count/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7909\n"
     ]
    }
   ],
   "source": [
    "X,y=get_class_names('dataset_cancer_v1/classificacao_binaria')\n",
    "benin,malignant=get_count('dataset_cancer_v1/classificacao_binaria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_fold=StratifiedKFold(n_splits=5)\n",
    "k_fold.get_n_splits(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_data(indexes,data,y):\n",
    "    return [data[i] for i in range(len(data)) if i in indexes],[y[i] for i in range(len(y)) if i in indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(x,y,transforms=None,batch_size=32,shuffle=True,batch_sampler=None):\n",
    "    return DataLoader(BreakHis(x,y,transforms=transforms),batch_size=batch_size,shuffle=shuffle,\n",
    "                     batch_sampler=batch_sampler,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_iter(model,optimizer,train_loader,accuracy_fn,f1_fn,loss_fn,writer,device):\n",
    "    running_loss=0.0\n",
    "    running_accuracy=0.0\n",
    "    running_f1_score=0.0\n",
    "    model.train()\n",
    "    for index,data in train_loader:\n",
    "        images=data['images'].to(device)\n",
    "        classes=data['classes'].to(device)\n",
    "        y_pred=model(images)\n",
    "        loss=loss_fn(\n",
    "            classes,\n",
    "            y_pred\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        accuracy=accuracy_fn(\n",
    "            y_pred.argmax(dim=-1),classes\n",
    "        )\n",
    "        f1_score=f1_fn(\n",
    "            y_pred.argmax(dim=-1),classes\n",
    "        )\n",
    "        writer.add_scalar('Loss/train',loss.item(),index)\n",
    "        writer.add_scalar('Accuracy/train',accuracy.item(),index)\n",
    "        writer.add_scalar(\"F1score/train\",f1_score.item(),index)\n",
    "        running_accuracy+=accuracy.item()\n",
    "        running_loss+=loss.item()\n",
    "        running_f1_score+=f1_score.item()\n",
    "\n",
    "        if index%100==0:\n",
    "            print(f\"loss {loss.item()} at index {index} accuracy {accuracy.item()} f1score {f1_score.item()}\")\n",
    "    return running_accuracy/len(train_loader),running_loss/len(train_loader),running_f1_score/len(train_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_iter(model,test_loader,accuracy_fn,f1_fn,loss_fn,writer,device):\n",
    "    model.eval()\n",
    "    running_loss=0.0\n",
    "    running_accuracy=0.0\n",
    "    running_f1_score=0.0\n",
    "    with torch.no_grad():\n",
    "        for index,data in test_loader:\n",
    "            images=data['images'].to(device)\n",
    "            classes=data['classes'].to(device)\n",
    "            y_pred=model(images)\n",
    "            loss=loss_fn(\n",
    "                classes,\n",
    "                y_pred\n",
    "            )\n",
    "            accuracy=accuracy_fn(\n",
    "            y_pred.argmax(dim=-1),classes\n",
    "            )\n",
    "            f1_score=f1_fn(\n",
    "                y_pred.argmax(dim=-1),classes\n",
    "            )\n",
    "            writer.add_scalar('Loss/train',loss.item(),index)\n",
    "            writer.add_scalar('Accuracy/train',accuracy.item(),index)\n",
    "            writer.add_scalar(\"F1score/train\",f1_score.item(),index)\n",
    "            running_accuracy+=accuracy.item()\n",
    "            running_loss+=loss.item()\n",
    "            running_f1_score+=f1_score.item()\n",
    "            \n",
    "            if index%100==0:\n",
    "                print(f\"loss {loss.item()} at index {index} accuracy {accuracy.item()} f1score {f1_score.item()}\")\n",
    "                \n",
    "    return running_accuracy/len(test_loader),running_loss/len(test_loader),running_f1_score/len(test_loader) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model,train_loader,test_loader,accuracy_fn,optimizer,f1_fn,loss_fn,epochs,fold):\n",
    "    total_history={}\n",
    "    for epoch in range(epochs):\n",
    "        history={}\n",
    "        writer=SummaryWriter(f\"fold_{fold}_epoch{epoch}\")\n",
    "        running_accuracy,running_loss,running_f1_score= train_one_iter(model,optimizer,train_loader,accuracy_fn,\n",
    "                                                                       f1_fn,loss_fn,writer)\n",
    "        \n",
    "        if 'train' in history.keys():\n",
    "            history['train']['Acc'].append(running_accuracy)\n",
    "            history['train']['loss'].append(running_loss)\n",
    "            history['train']['f1'].append(running_f1_score)\n",
    "        else:\n",
    "            history['train']={\n",
    "                'Acc':[running_accuracy],\n",
    "                'loss':[running_loss],\n",
    "                'f1':[running_f1_score]\n",
    "            }\n",
    "        running_accuracy,running_loss,running_f1_score= test_one_iter(\n",
    "            model,test_loader,accuracy_fn,f1_fn,loss_fn,writer\n",
    "        )\n",
    "        if 'test' in history.keys():\n",
    "            history['test']['Acc'].append(running_accuracy)\n",
    "            history['test']['loss'].append(running_loss)\n",
    "            history['test']['f1'].append(running_f1_score)\n",
    "        else:\n",
    "            history['test']={\n",
    "                'Acc':[running_accuracy],\n",
    "                'loss':[running_loss],\n",
    "                'f1':[running_f1_score]\n",
    "            }\n",
    "        total_history[f\"{epoch}_{fold}\"]=history\n",
    "    \n",
    "    return total_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m=\u001b[39mAttention_exploitation(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m'\u001b[39m,num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m optimizer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-1\u001b[39m,weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m      5\u001b[0m cosine_anealing\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingWarmRestarts(optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m      6\u001b[0m                                                                      T_0\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      7\u001b[0m                                                                      eta_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m,\n\u001b[0;32m      8\u001b[0m                                                                      last_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      9\u001b[0m                                                                      verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\madhava\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:905\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    889\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \n\u001b[0;32m    891\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\madhava\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\madhava\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 797 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\madhava\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\madhava\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\madhava\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:905\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    889\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \n\u001b[0;32m    891\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\madhava\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:247\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m    246\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 247\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[0;32m    251\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model=Attention_exploitation(type='base',num_classes=2)\n",
    "model=model.to(device=device)\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=1e-1,weight_decay=1e-4)\n",
    "\n",
    "cosine_anealing=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer=optimizer,\n",
    "                                                                     T_0=2,\n",
    "                                                                     eta_min=1e-5,\n",
    "                                                                     last_epoch=10,\n",
    "                                                                     verbose=True)\n",
    "\n",
    "\n",
    "epochs=10\n",
    "accuracy_fn=BinaryAccuracy()\n",
    "f1_score=BinaryF1Score()\n",
    "normed_weights=[1-benin,1-malignant]\n",
    "loss_fn=torch.nn.CrossEntropyLoss(weight=torch.FloatTensor(normed_weights).cuda())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247\n",
      "247\n",
      "247\n",
      "247\n",
      "247\n"
     ]
    }
   ],
   "source": [
    "def \n",
    "sss=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n",
    "for i,(train,test) in enumerate(sss.split(X,y)):\n",
    "    train_idx=list(train)\n",
    "    test_idx=list(test)\n",
    "    \n",
    "    train_data_X,train_data_y=give_data(train_idx,X,y)\n",
    "    test_data_X,test_data_y=give_data(test_idx,X,y)\n",
    "\n",
    "    skf=StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
    "    for indexes,(training,validation) in enumerate(skf.split(train_data_X,train_data_y)):\n",
    "        train_data_X,train_data_y=give_data(list(training),X,y)\n",
    "        validation_data_x,validation_data_y=give_data(list(validation),X,y)\n",
    "        train_loader= get_loader(x=X,y=y,transforms=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ]))\n",
    "        \n",
    "        test_loader=get_loader(x=X,y=y,transforms=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ]))\n",
    "        total_history=training_loop(model,train_loader,test_loader=test_loader,\n",
    "                                    accuracy_fn=accuracy_fn,optimizer=optimizer,f1_fn=f1_score,loss_fn=loss_fn,\n",
    "                                    epochs=epochs,fold=i,device=device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorA=torch.tensor([1,1,1],dtype=torch.float32)\n",
    "tensorB=torch.tensor([[0.99,2.44],[1.22,2.44],[2.22,4]],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorB.softmax(dim=-1).argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BinaryAccuracy()(tensorA,tensorB.argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
