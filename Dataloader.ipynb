{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from attention_tuning import Attention_exploitation\n",
    "from PIL import Image\n",
    "from torchmetrics.classification import BinaryAccuracy,BinaryF1Score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm,trange\n",
    "import pickle\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreakHis(Dataset):\n",
    "    def __init__(self,X,y,transforms=None):\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.transforms=transforms\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        path=self.X[idx]\n",
    "        image=Image.open(path)\n",
    "        image=self.transforms(image=np.array(image))\n",
    "        \n",
    "        return{\n",
    "           'Image' :image['image'],\n",
    "           'class':self.y[idx],\n",
    "           'magnification':self.X[idx].split('\\\\')[-2]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_names(directory):\n",
    "    file_paths=glob.glob(directory+\"/**/*.png\",recursive=True)\n",
    "    classes=[get_class_properties(i) for i in file_paths]\n",
    "    return file_paths,classes\n",
    "def get_class_properties(directory):\n",
    "    directory=directory.replace('\\\\','/')\n",
    "    d=directory.split('/')[-2]\n",
    "    return 0 if d=='benign' else 1\n",
    "def get_count(directory):\n",
    "    file_paths=[x.replace('\\\\','/') for x in glob.glob(directory+\"/**/*.png\",recursive=True)]\n",
    "    total=len(file_paths)\n",
    "    benin_count=len([i for i in file_paths if i.split('/')[-2]==\"benign\"])\n",
    "    print(total)\n",
    "    malignant_count=total-benin_count\n",
    "    return benin_count/total,malignant_count/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7909\n"
     ]
    }
   ],
   "source": [
    "X,y=get_class_names('dataset_cancer_v1/classificacao_binaria')\n",
    "benin,malignant=get_count('dataset_cancer_v1/classificacao_binaria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_fold=StratifiedKFold(n_splits=5)\n",
    "k_fold.get_n_splits(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_data(indexes,data,y):\n",
    "    return [data[i] for i in range(len(data)) if i in indexes],[y[i] for i in range(len(y)) if i in indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(x,y,transforms=None,batch_size=32,shuffle=True,batch_sampler=None):\n",
    "    return DataLoader(BreakHis(x,y,transforms=transforms),batch_size=batch_size,shuffle=shuffle,\n",
    "                     batch_sampler=batch_sampler,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(tensor):\n",
    "    tensor=tensor.detach().cpu().numpy()\n",
    "    lists=[]\n",
    "    for i in tensor:\n",
    "        if i==0:\n",
    "            lists.append([1,0])\n",
    "        else:\n",
    "            lists.append([0,1])\n",
    "    return torch.tensor(lists,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_iter(model,optimizer,train_loader,accuracy_fn,f1_fn,loss_fn,writer,device,scedular,current_epoch):\n",
    "    running_loss=0.0\n",
    "    running_accuracy=0.0\n",
    "    running_f1_score=0.0\n",
    "    model.train()\n",
    "    for index,data in enumerate(train_loader):\n",
    "        images=data['Image'].to(device)\n",
    "        classes=convert_to_list(data['class']).to(device)\n",
    "        y_pred=model(images)\n",
    "        loss=loss_fn(\n",
    "            y_pred,\n",
    "            classes\n",
    "        )\n",
    "        loss.backward()\n",
    "        if torch.isnan(y_pred).any():\n",
    "                print(data,y_pred)\n",
    "                print(\"nan found\")\n",
    "                assert False\n",
    "\n",
    "        if torch.isnan(classes).any():\n",
    "                print(\"nan found\")\n",
    "                assert False\n",
    "\n",
    "        optimizer.step()\n",
    "        scedular.step(current_epoch + index / len(train_loader))\n",
    "        accuracy=accuracy_fn(\n",
    "            y_pred.argmax(dim=-1),classes.argmax(dim=-1)\n",
    "        )\n",
    "        f1_score=f1_fn(\n",
    "            y_pred.argmax(dim=-1),classes.argmax(dim=-1)\n",
    "        )\n",
    "        writer.add_scalar('Loss/train',loss.item(),index)\n",
    "        writer.add_scalar('Accuracy/train',accuracy.item(),index)\n",
    "        writer.add_scalar(\"F1score/train\",f1_score.item(),index)\n",
    "        running_accuracy+=accuracy.item()\n",
    "        running_loss+=loss.item()\n",
    "        running_f1_score+=f1_score.item()\n",
    "\n",
    "        if index%100==0:\n",
    "            print(f\"loss {loss.item()} at index {index} accuracy {accuracy.item()} f1score {f1_score.item()}\")\n",
    "    return running_accuracy/len(train_loader),running_loss/len(train_loader),running_f1_score/len(train_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_iter(model,test_loader,accuracy_fn,f1_fn,loss_fn,writer,device):\n",
    "    model.eval()\n",
    "    running_loss=0.0\n",
    "    running_accuracy=0.0\n",
    "    running_f1_score=0.0\n",
    "    with torch.no_grad():\n",
    "        for index,data in enumerate(test_loader):\n",
    "            images=data['Image'].to(device)\n",
    "            classes=convert_to_list(data['class']).to(device)\n",
    "            y_pred=model(images)\n",
    "            loss=loss_fn(\n",
    "                classes,\n",
    "                y_pred.softmax(dim=-1)\n",
    "            )\n",
    "            accuracy=accuracy_fn(\n",
    "            y_pred.argmax(dim=-1),classes.argmax(dim=-1)\n",
    "            )\n",
    "            f1_score=f1_fn(\n",
    "                y_pred.argmax(dim=-1),classes.argmax(dim=-1)\n",
    "            )\n",
    "            if torch.isnan(y_pred).any():\n",
    "                print(\"nan found\")\n",
    "                print(data)\n",
    "                assert False\n",
    "\n",
    "            if torch.isnan(classes).any():\n",
    "                print(\"nan found\")\n",
    "                assert False\n",
    "            writer.add_scalar('Loss/train',loss.item(),index)\n",
    "            writer.add_scalar('Accuracy/train',accuracy.item(),index)\n",
    "            writer.add_scalar(\"F1score/train\",f1_score.item(),index)\n",
    "            running_accuracy+=accuracy.item()\n",
    "            running_loss+=loss.item()\n",
    "            running_f1_score+=f1_score.item()\n",
    "            \n",
    "            if index%100==0:\n",
    "                print(f\"loss {loss.item()} at index {index} accuracy {accuracy.item()} f1score {f1_score.item()}\")\n",
    "                \n",
    "    return running_accuracy/len(test_loader),running_loss/len(test_loader),running_f1_score/len(test_loader) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model,train_loader,test_loader,accuracy_fn,optimizer,f1_fn,loss_fn,epochs,fold,device,secdular):\n",
    "    total_history={}\n",
    "    os.makedirs(name=f\"ModelWeights/weight_fold_{fold}\",exist_ok=True)\n",
    "    for epoch in trange(epochs):\n",
    "        history={}\n",
    "        writer=SummaryWriter(f\"fold_{fold}_epoch{epoch}\")\n",
    "        running_accuracy,running_loss,running_f1_score= train_one_iter(model,optimizer,train_loader,accuracy_fn,\n",
    "                                                                       f1_fn,loss_fn,writer,device,secdular,epoch)\n",
    "        \n",
    "        if 'train' in history.keys():\n",
    "            history['train']['Acc'].append(running_accuracy)\n",
    "            history['train']['loss'].append(running_loss)\n",
    "            history['train']['f1'].append(running_f1_score)\n",
    "        else:\n",
    "            history['train']={\n",
    "                'Acc':[running_accuracy],\n",
    "                'loss':[running_loss],\n",
    "                'f1':[running_f1_score]\n",
    "            }\n",
    "        running_accuracy,running_loss,running_f1_score= test_one_iter(\n",
    "            model,test_loader,accuracy_fn,f1_fn,loss_fn,writer,device\n",
    "        )\n",
    "        secdular.step(running_loss)\n",
    "        if 'test' in history.keys():\n",
    "            history['test']['Acc'].append(running_accuracy)\n",
    "            history['test']['loss'].append(running_loss)\n",
    "            history['test']['f1'].append(running_f1_score)\n",
    "        else:\n",
    "            history['test']={\n",
    "                'Acc':[running_accuracy],\n",
    "                'loss':[running_loss],\n",
    "                'f1':[running_f1_score]\n",
    "            }\n",
    "        total_history[f\"{epoch}_{fold}\"]=history\n",
    "        \n",
    "        for i in history.keys():\n",
    "            for j in history[i].keys():\n",
    "                print(f\"Epoch {epoch+1} {i}:{j},{sum(history[i][j])/len(history[i][j])}\")\n",
    "        \n",
    "        with open(f'total_history_fold_{fold}_epoch_{epoch}.pkl','wb') as f:\n",
    "            pickle.dump(history,f)\n",
    "        \n",
    "        torch.save(model.parameters(),f\"ModelWeights/weight_fold_{fold}/epoch_{epoch}.pt\")\n",
    "\n",
    "    return total_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs=10\n",
    "accuracy_fn=BinaryAccuracy().to(device)\n",
    "f1_score=BinaryF1Score().to(device)\n",
    "normed_weights=[1-benin,1-malignant]\n",
    "loss_fn=torch.nn.BCEWithLogitsLoss(weight=torch.FloatTensor(normed_weights).to(device=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters are frozen!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f23ad7526a4dea9546d0654d4673cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.33903369307518005 at index 0 accuracy 0.625 f1score 0.7692307829856873\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 55\u001b[0m\n\u001b[0;32m     24\u001b[0m train_loader\u001b[38;5;241m=\u001b[39m get_loader(x\u001b[38;5;241m=\u001b[39mX,y\u001b[38;5;241m=\u001b[39my,transforms\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     25\u001b[0m     A\u001b[38;5;241m.\u001b[39mOneOf([\n\u001b[0;32m     26\u001b[0m         A\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     ToTensorV2(),  \u001b[38;5;66;03m# Convert to PyTorch Tensor\u001b[39;00m\n\u001b[0;32m     38\u001b[0m ]))\n\u001b[0;32m     40\u001b[0m test_loader\u001b[38;5;241m=\u001b[39mget_loader(x\u001b[38;5;241m=\u001b[39mX,y\u001b[38;5;241m=\u001b[39my,transforms\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     41\u001b[0m     A\u001b[38;5;241m.\u001b[39mOneOf([\n\u001b[0;32m     42\u001b[0m         A\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m     ToTensorV2(),  \u001b[38;5;66;03m# Convert to PyTorch Tensor\u001b[39;00m\n\u001b[0;32m     54\u001b[0m ]))\n\u001b[1;32m---> 55\u001b[0m total_history\u001b[38;5;241m=\u001b[39m\u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m                            \u001b[49m\u001b[43maccuracy_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccuracy_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mf1_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf1_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43msecdular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcosine_anealing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"for i in train_loader:\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    print(i)\"\"\"\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(model, train_loader, test_loader, accuracy_fn, optimizer, f1_fn, loss_fn, epochs, fold, device, secdular)\u001b[0m\n\u001b[0;32m      5\u001b[0m history\u001b[38;5;241m=\u001b[39m{}\n\u001b[0;32m      6\u001b[0m writer\u001b[38;5;241m=\u001b[39mSummaryWriter(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_epoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m running_accuracy,running_loss,running_f1_score\u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43maccuracy_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                                               \u001b[49m\u001b[43mf1_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43msecdular\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m history\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m     11\u001b[0m     history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAcc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(running_accuracy)\n",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m, in \u001b[0;36mtrain_one_iter\u001b[1;34m(model, optimizer, train_loader, accuracy_fn, f1_fn, loss_fn, writer, device, scedular, current_epoch)\u001b[0m\n\u001b[0;32m     10\u001b[0m loss\u001b[38;5;241m=\u001b[39mloss_fn(\n\u001b[0;32m     11\u001b[0m     y_pred,\n\u001b[0;32m     12\u001b[0m     classes\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(y_pred)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28mprint\u001b[39m(data,y_pred)\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sss=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n",
    "for i,(train,test) in enumerate(sss.split(X,y)):\n",
    "    train_idx=list(train)\n",
    "    test_idx=list(test)\n",
    "    \n",
    "    train_data_X,train_data_y=give_data(train_idx,X,y)\n",
    "    test_data_X,test_data_y=give_data(test_idx,X,y)\n",
    "\n",
    "    skf=StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
    "    for indexes,(training,validation) in enumerate(skf.split(train_data_X,train_data_y)):\n",
    "        model=Attention_exploitation(type='base',num_classes=2)\n",
    "        model=model.to(device=device)\n",
    "        optimizer=torch.optim.SGD(model.parameters(),\n",
    "                           lr=1e-3,weight_decay=1e-3)\n",
    "        cosine_anealing=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer=optimizer,\n",
    "                                                                     T_0=2,\n",
    "                                                                     eta_min=1e-6,\n",
    "                                                                     last_epoch=-1,\n",
    "                                                                     verbose=False)\n",
    "\n",
    "        train_data_X,train_data_y=give_data(list(training),X,y)\n",
    "        validation_data_x,validation_data_y=give_data(list(validation),X,y)\n",
    "\n",
    "        train_loader= get_loader(x=X,y=y,transforms=A.Compose([\n",
    "            A.OneOf([\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "                 A.RandomRotate90(p=0.5)\n",
    "            ],p=0.1),\n",
    "            A.OneOf([\n",
    "            A.GaussianBlur(p=0.5),\n",
    "            A.ChannelShuffle(p=0.5)\n",
    "            ],p=0.1), \n",
    "            A.Normalize(),  # Normalization\n",
    "            ToTensorV2(),  # Convert to PyTorch Tensor\n",
    "        ]))\n",
    "        \n",
    "        test_loader=get_loader(x=X,y=y,transforms=A.Compose([\n",
    "            A.OneOf([\n",
    "                A.Sequential([\n",
    "                    A.HorizontalFlip(p=0.5),\n",
    "                    A.VerticalFlip(p=0.5),\n",
    "                    A.RandomRotate90(p=0.5)\n",
    "                ])\n",
    "            ],p=0.125),\n",
    "            A.OneOf([\n",
    "            A.GaussianBlur(p=0.5),\n",
    "            A.ChannelShuffle(p=0.5)\n",
    "            ],p=0.125), \n",
    "            A.Normalize(),  # Normalization\n",
    "            ToTensorV2(),  # Convert to PyTorch Tensor\n",
    "        ]))\n",
    "        total_history=training_loop(model,train_loader,test_loader=test_loader,\n",
    "                                    accuracy_fn=accuracy_fn,optimizer=optimizer,f1_fn=f1_score,loss_fn=loss_fn,\n",
    "                                    epochs=epochs,fold=i,device=device,secdular=cosine_anealing)\n",
    "        \n",
    "        \"\"\"for i in train_loader:\n",
    "            print(i)\"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([[1,0]]).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Sigmoid.__init__() got an unexpected keyword argument 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m(torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m5.5\u001b[39m,\u001b[38;5;241m4.3\u001b[39m]]))\n",
      "File \u001b[1;32mc:\\Users\\madhava\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:445\u001b[0m, in \u001b[0;36mModule.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# Backward compatibility: no args used to be allowed when call_super_init=False\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_super_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(kwargs):\n\u001b[1;32m--> 445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.__init__() got an unexpected keyword argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))))\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_super_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(args):\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.__init__() takes 1 positional argument but \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m were\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    450\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m given\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: Sigmoid.__init__() got an unexpected keyword argument 'dim'"
     ]
    }
   ],
   "source": [
    "torch.nn.Sigmoid(dim=-1)(torch.tensor([[5.5,4.3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
